---
title: "Rules of engagement before touching a server"
date: 2022-12-28T07:58:44+01:00
draft: true
tags: ["development", "batch", "quality"]
---

As developer you might receive an xls, csv and do something about it, like importing/updating your system or something your app integrates with to reconcile.

Since it's a complex topic, you might want to keep in your head some common requirements that make such operation a success.

- **traceability**
  - know which data is coming from where (ex file_name, line)
  - try to keep the initial data untouched
    - ex add columns to panda dataframe with cleaned/calculated/... final fields (ex show there was spaces in the inital value, that your batch cleaned up, this will also make stats field "before" "after" easier to analyse)
  - if possible mark the imported (ex if the model allow some notes, add a note with "import from $file_name $line"
- **visible to business**
  - keep stats of what gets in and what is rejected
    - missing required fields,
    - unhandled mapped values
    - situation changes since your offline mapping/matching algorithm (ex we want to import lab results and there's now a lab result cfr next point)
  - ideally configurable based on excel or csv so you can crowd source the mapping or review of the mapping for [entity resolution](https://towardsdatascience.com/an-introduction-to-entity-resolution-needs-and-challenges-97fba052dde5)
- **idempotence**
  - don't create duplicate records (some system allows to pre assign uuid upfront like dhis2)
  - don't fail on trying to re delete a record already deleted
- **auditable**
  - logs produced on success, on error, on rejection of the data
  - is some errors occured you might be saved by these logs
  - if you can add columns with api or web app urls to see the import record this will save you time
- **performance**
  - don't expect you will import millions of records in 1 api call
  - find a tradeoff between cpu and memory usage (think paginagion)
  - think about ways to allow some parallelisation
    - ex run per slice 0..100000 100000..2000000 ...
    - load data per pages not 1 by 1 (to avoid select n+1)
    - warn : if you update your data, the "filter" might get influenced breaking pagination
  - for patterns/abstraction you might be interested how [spring batch](https://terasoluna-batch.github.io/guideline/5.0.0.RELEASE/en/Ch02_SpringBatchArchitecture.html#Ch02_SpringBatchArch_Detail_BusinessLogic) wants you to structure the code (chuncks, slices,...)
- **robust, interruptible, resumable**
  - add validations on the data (ex for enums)
  - add normalisations (ex strip blanks, strip double spaces, these weird "office" [chars](https://stackoverflow.com/questions/10294032/python-replace-typographical-quotes-dashes-etc-with-their-ascii-counterparts) for quote, simple quote : `Centre de santé de l’acobo` `Centre de santé “Tika”`, ...)
  - if one record out millions doesn't make it, you should perhaps just catch the exception, log it, proceed with the next record
  - some mass effect you might end up breaking
    - your script (accumulation => mem full),
    - the server (timeout, memory leak, logs,...),
    - the db (storage full)
  - so you want to be able to re run it (cfr indempotent) or at least resume the batch at the last processed record
- **don't wait too long before really doing stuff in prod**
  - once you have tested in a staging env your
  - if the logic is complex new cases might arise (new data added) that your staging test won't discover
- **don't expect 100% and a waterfall process**
  - expect a lot of records to be excluded
    - human data entry and free text make generally a lot waste
  - expect a lot of iteration on data cleaning / normalization / matching / entity resolution
    - business might need to get into it (mapping, sane default value for stuff that are missing, decide on too complex cases (reject or propose workaround) )
  - perhaps once the import is done, you will detect problem generated by the import
    - ex report status set to "to process" instead of "completed", poluting todo list of the data clerks
    - ex some field were "omitted" but finally has a real purpose for data clerks
    - you might need fixes on the imported data
- **It's perhaps one time code but it needs to be good**
  - unit test (ex normalisation, some matching, entity resolution cases)
  - integration test on "smaller" data (verify external api calls)
  - depending on the app where you are importing the data, it might be too permisive and create more problems
- **safetynets**
  - make a backup (even if you won't probably be able to restore it, you can at least find somewhere the situation "before" your mass import/update/delete)

Overall, when processing batch CSV files, it's important to think about the entire data pipeline, from data collection and preparation to processing, analysis, and reporting, additional run. By considering all these aspects, you can create a robust, efficient, and secure process for processing large data sets.
